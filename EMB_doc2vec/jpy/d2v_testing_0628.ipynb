{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0100|Live,animals\t\r\n",
      "0200|Meat,meat,offal\t\r\n",
      "0300|Fish,crustaceans,molluscs,aquatic,invertebrates\t\r\n",
      "0400|Dairy,eggs,honey,animal\t\r\n",
      "0500|animal,origin\r\n",
      "0600|trees,plants,bulbs,roots,flowers,ornamental,foliage.\t\r\n",
      "0700|vegetables,roots,tubers\t\r\n",
      "0800|fruit,nuts,peel,citrus,fruit,melons\t\r\n",
      "0900|Coffee,tea,mate and spices\t\r\n",
      "1000|Cereals"
     ]
    }
   ],
   "source": [
    "!cat \"../data/d2v.hscode.sample\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Mockup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_docs = []\n",
    "with open(\"../data/d2v.hscode.sample\", \"r\") as f:\n",
    "    for line in f.readlines(): \n",
    "        for dup in range(10000):\n",
    "            line = line.strip()\n",
    "            hscode, words = line.split(\"|\")\n",
    "            words = list(set([word.lower() for word in words.split(\",\")]))\n",
    "#             print(f\"HSCODE : {hscode} | WORDS : {words}\")\n",
    "            tagged_docs.append(TaggedDocument(words=words, tags=[str(dup)+hscode]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tagged_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DVmodel = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=1)\n",
    "DVmodel.build_vocab(tagged_docs)\n",
    "DVmodel.train(tagged_docs, total_examples=len(tagged_docs), epochs=500)\n",
    "DVmodel.save(\"../models/test_model_0628/my_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "def load_model(path):\n",
    "    return Doc2Vec.load(path)\n",
    "pretrained_d2v = load_model(\"../models/test_model_0628/my_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(model, key):\n",
    "    return model.docvecs[key]\n",
    "\n",
    "def get_doctag_and_vectors(model, check=False):\n",
    "    doctags = model.docvecs.index2entity\n",
    "    vectors = model.docvecs.vectors_docs\n",
    "    if check == True:\n",
    "        for _index, _id in enumerate(doctags):\n",
    "            assert np.array_equal(model.docvecs[_id], vectors[_index]), \"NONO\"\n",
    "    return doctags, vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctags, all_vectors = get_doctag_and_vectors(pretrained_d2v, check = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Tags to Two sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tags_with_two_set(model, a_tags, b_tags=None):\n",
    "\n",
    "    def _get_doctag_and_vector(model, tags):        \n",
    "        _doctags = model.docvecs.index2entity\n",
    "        _doctag_index = [_doctags.index(tag) for tag in tags]\n",
    "        _vectors = model.docvecs.vectors_docs[_doctag_index]\n",
    "        return tags, _vectors\n",
    "    \n",
    "    a_doctags, a_vectors = _get_doctag_and_vector(model, a_tags)\n",
    "    \n",
    "    if b_tags == None:\n",
    "        b_tags = [b_tag for b_tag in model.docvecs.index2entity if b_tag not in a_doctags]\n",
    "    \n",
    "    b_doctags, b_vectors = _get_doctag_and_vector(model, b_tags)\n",
    "\n",
    "    return a_doctags, a_vectors, b_doctags, b_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_tag = ['0100','0400']\n",
    "q_tag, q_vec, v_tag, v_vec = split_tags_with_two_set(pretrained_d2v, q_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['0100', '0400'],\n",
       " ['0200', '0300', '0500', '0600', '0700', '0800', '0900', '1000'])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_tag, v_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9571379 , 0.97786653, 0.9517958 , 0.97486556, 0.95984334,\n",
       "        0.96753204, 0.9672515 , 0.9509969 ],\n",
       "       [0.9750438 , 0.98059654, 0.9755604 , 0.98962146, 0.97363627,\n",
       "        0.9829849 , 0.97819394, 0.94849193]], dtype=float32)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cosine_matrix(q_vec, v_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get cosine Matrix\n",
    "- it is totally same 'docvecs.most_similar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def get_cosine_matrix(a, b):\n",
    "    return cosine_similarity(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.091769933700562\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "st = time.time()\n",
    "cos_mat = get_cosine_matrix(all_vectors, all_vectors)\n",
    "print(time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 100000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "958.1089019775391\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "np.argsort(cos_mat[0], axis=-1)\n",
    "print((time.time()-st)*len(cos_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "loaded_model = KeyedVectors.load_word2vec_format(\"../models/pretrained_w2v/GoogleNews-vectors-negative300-SLIM.bin\", binary=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "d2v_plain = gensim.models.doc2vec.Doc2Vec(vector_size=300, min_count=1)\n",
    "d2v_plain.build_vocab(tagged_docs)\n",
    "d2v_plain.train(tagged_docs, total_examples=len(tagged_docs), epochs=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"word 'foliage.' not in vocabulary\"\n",
      "\"word 'mate and spices' not in vocabulary\"\n"
     ]
    }
   ],
   "source": [
    "transfer_d2v = gensim.models.doc2vec.Doc2Vec(vector_size=300, min_count=1)\n",
    "transfer_d2v.build_vocab(tagged_docs)\n",
    "\n",
    "def init_w2v_to_d2v(d2v_model, w2v_model):\n",
    "    for k in d2v_model.wv.index2word:\n",
    "        try:\n",
    "            transfer_d2v.wv[k] = w2v_model[k]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "init_w2v_to_d2v(transfer_d2v, loaded_model)\n",
    "transfer_d2v.wv['animal']\n",
    "transfer_d2v.train(tagged_docs, total_examples=len(tagged_docs), epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0100|Live,animals\t\r\n",
      "0200|Meat,meat,offal\t\r\n",
      "0300|Fish,crustaceans,molluscs,aquatic,invertebrates\t\r\n",
      "0400|Dairy,eggs,honey,animal\t\r\n",
      "0500|animal,origin\r\n",
      "0600|trees,plants,bulbs,roots,flowers,ornamental,foliage.\t\r\n",
      "0700|vegetables,roots,tubers\t\r\n",
      "0800|fruit,nuts,peel,citrus,fruit,melons\t\r\n",
      "0900|Coffee,tea,mate and spices\t\r\n",
      "1000|Cereals"
     ]
    }
   ],
   "source": [
    "!cat \"../data/d2v.hscode.sample\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0800']\n",
      "['0600', '0700', '0300', '0500', '0900', '0400', '0200', '0100', '1000']\n",
      "['0800']\n",
      "['0300', '0500', '0600', '0700', '0900', '0400', '0200', '0100', '1000']\n"
     ]
    }
   ],
   "source": [
    "for model in [d2v_plain, transfer_d2v]:\n",
    "    a_tag, a_vec, b_tag, b_vec = split_tags_with_two_set(model, ['0800'])\n",
    "    _index = np.argsort(get_cosine_matrix(a_vec, b_vec), axis=-1)[0]\n",
    "    print(a_tag)\n",
    "    print([b_tag[_i] for _i in _index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "t = np.array([1]*12)\n",
    "print(t)\n",
    "def __split_indices(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n",
      "[1 1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "for i in __split_indices(range(len(t)), 10):\n",
    "    print(t[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "with open(\"/home/jack/.bashrc\", \"r\") as f:\n",
    "    print(len(f.readline()))\n",
    "    lines = random.sa.mple(f.readlines(),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136 /home/jack/.bashrc\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l \"/home/jack/.bashrc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['데모 영상으로 제작된 버전은 다음 레파지토리에 있습니다. <br> https://github.com/BM-K/KoSentenceBERT_V2',\n",
       " \"'치타가 들판을 가로 질러 먹이를 쫓는다.']\",\n",
       " 'ETRI KorBERT는 transformers 2.4.1 ~ 2.8.0에서만 동작하고 Sentence-BERT는 3.1.0 버전 이상에서 동작하여 라이브러리를 수정하였습니다. <br>',\n",
       " '```',\n",
       " '']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[_lines.strip() for _lines in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = len(open(\"/home/jack/.bashrc\").readlines(  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
